# DSI_projects
Data Science Projects completed from NOV2020 - FEB2021


Project 1 Executive Summary :

Let's pretend I work for a the University of Colorado - Boulder in admissions. We have a lot of applications and we cannot accept all that would like to come 
to CU Boulder or we would be overcrowded. We are going to use the SAT to selectivly decide a lower threshold of students we will not consider unless there
have reasonable exceptionalities. We also want to split up the students based on their prospective majors and have some standards for entry into the departments 
of study with in the college for those students that will right away begin work in their prospective major.
I want to see how people perform in the math and reading/writing section of the SAT in regards to their respective majors of interest. Perhaps these scores may 
be of more importance to specific colleges, such as math to the engineering department. It is a matter of curiousity for me to see if their strengths on test 
scores are a reflection of their major choices.

Thus with the available data, we are tasked with finding the scores spread of last years test takers with consideration for their declared college majors and for 
setting up a rubric of expectations for the college departments. We will have to bunch up the majors into selected colleges and we will need to select the minimum 
test scores required for entry into individual colleges based of the entry standards of the university.

Project 2 Executive Summary :

I am now a data scientist working for the Real Estate Appraisers Coalition of Ames Iowa.
We have a lot of homes in the region that need to be quickly appraised for government and educational purposes.
We have data from the Ames Iowa Assessor’s Office used in computing values for individual residential properties 
sold in Ames, IA from 2006 to 2010. We are going to create a model that predicts prices off home assessment 
data entries, and in doing so we will determine what factors make the most significant impacts on the model
and fine tune the model to be more effective. Furthermore, we will test and verify these results on a competitive website for 
data scientists (keggle) and come to an analysis on how effective the model actually is.
Essentially we are asking the question:
What factors really matter in determining a fair market value for a home in Ames Iowa?

Project 3 Executive Summary : 

Hello there. My name is Cm April, nice to make your acquaintance. In this notebook I will demonstrate how to scrape data off the web using a web 
application programming interface or web API. In our case we are using the Pushshift's reddit API to extract submissions. 
In addition to this, we will use classification models and natural language processing (NLP) to determine which subreddit the language came from. 
This can be useful in many applications of technology that use text and/or verbal language to understand humans.
It is important to learn if you are going to explore a career in data science like I am. I hope this lesson is, as I am learning as I go, as 
insightful to you as it has been for me. This analysis will take the following steps to classify reddit submissions of two of my 
all-time favorite science fiction series.

Using Pushshift's API, we will collect posts from two subreddits, Star Wars and Star Trek.
Then we will use NLP to train a classifier on determining which subreddit a given post came from. This is a binary classification problem.

Project 4 Executive Summary :

This was my first group project. We used git to and slack to collaborate and in just a few hours we picked a topic, perforemed basic eda and cleaning, 
models, and visualizations. The goal of our project was to build a classifier which will correctly predict the outcome for a shelter animal based on 
select characteristics of animals. 
The problem is a multi-class classification with the labels being: 'Adoption', 'Return_to_owner','Transfer', 'Euthanasia', and 'Died'.

Project 5 Executive Summary :

2020 was the most active fire season in the Western United States’s recorded history. California had the single worst fire season in it’s history, 
while Arizona had the worst in a decade. Oregon had its most destructive fire season meanwhile Washington and Colorado had several of their all time 
largest wildfires. Overall 10.2 million acres of land went up in flame and 46 people lost their lives. 13,887 buildings were destroyed and the total
cost is upwards of 19.88 billion USD. It is evident that fire is a clear and present danger in the western united states.

The global atmospheric monitoring satellite Copernicus has recorded CO2 emissions from the 2020 fires and noted that 
“The fires are also emitting lots of smoke and pollution into the atmosphere; those in California and Oregon have already emitted far more carbon in 2020
than in any other year since CAMS records begin in 2003” - CAMS monitors smoke release from devastating US wildfires | Copernicus. We decided to investigate 
the relationship between weather data (precipitation, temperatures, and drought) and the occurrence of fires, and to attempt building a model which would
predict the destructive sizes of wildfire to help prevent the associated damage for our communities.

Project 6 Executive Summary :

I'm interested in AI and deep neural networks so for this project I expermented with gpt-2. I trained gpt-2 on stat trek scripts and created original stories 
from scratch. 

